{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f142ba1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jdbc:postgresql://172.22.0.9:5432/spark_db\n",
      "+--------------------+\n",
      "|             version|\n",
      "+--------------------+\n",
      "|PostgreSQL 15.14 ...|\n",
      "+--------------------+\n",
      "\n",
      "model='gpt-oss:latest' created_at='2026-02-01T17:12:37.105412813Z' done=True done_reason='stop' total_duration=5174728681 load_duration=3074788600 prompt_eval_count=71 prompt_eval_duration=509469026 eval_count=107 eval_duration=1589893404 message=Message(role='assistant', content='Yes—I’m here and ready to help! What do you need?', thinking='The user asks: \"Are you working?\" That is ambiguous. Could be a joke about AI. Might want to respond: \"Yes, I\\'m here, how can I help?\" The user might be asking whether I am currently functioning or \"working\" in the sense of being employed. So answer politely. It\\'s an informal question. Should be short and helpful. Probably mention I\\'m an AI and always ready to assist.', images=None, tool_name=None, tool_calls=None) logprobs=None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyiceberg.catalog import load_catalog\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import ollama\n",
    "import torch\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Get variables from environment\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "db_user = os.getenv('DB_USER')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "jdbc_jar = os.getenv('JDBC_JAR_PATH')\n",
    "os.makedirs(\"/tmp/iceberg-warehouse\", exist_ok=True)\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.jars\", jdbc_jar) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "spark_db_conf = {\n",
    "    \"url\":f\"jdbc:postgresql://{db_host}:{db_port}/{db_name}\",\n",
    "    \"user\":db_user,\n",
    "    \"pw\": db_password\n",
    "    \n",
    "}\n",
    "\n",
    "print(spark_db_conf['url'])\n",
    "\n",
    "pgsql_details = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", spark_db_conf['url']) \\\n",
    "    .option(\"query\", \"SELECT version()\") \\\n",
    "    .option(\"user\", spark_db_conf['user']) \\\n",
    "    .option(\"password\", spark_db_conf['pw']) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "    \n",
    "\n",
    "catalog = load_catalog(\n",
    "    \"postgres\",\n",
    "    **{\n",
    "        \"type\": \"sql\",\n",
    "        \"uri\": f\"postgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\",\n",
    "        \"warehouse\": \"/tmp/iceberg-warehouse\"\n",
    "    }\n",
    ")\n",
    "    \n",
    "pgsql_details.show()\n",
    "\n",
    "response = ollama.chat(model='gpt-oss:latest', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Are you working?',\n",
    "    },\n",
    "], keep_alive=0)\n",
    "\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcbcce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method chat in module ollama._client:\n",
      "\n",
      "chat(model: str = '', messages: Optional[Sequence[Union[Mapping[str, Any], ollama._types.Message]]] = None, *, tools: Optional[Sequence[Union[Mapping[str, Any], ollama._types.Tool, Callable]]] = None, stream: bool = False, think: Union[bool, Literal['low', 'medium', 'high'], NoneType] = None, logprobs: Optional[bool] = None, top_logprobs: Optional[int] = None, format: Union[Literal['', 'json'], dict[str, Any], NoneType] = None, options: Union[Mapping[str, Any], ollama._types.Options, NoneType] = None, keep_alive: Union[float, str, NoneType] = None) -> Union[ollama._types.ChatResponse, collections.abc.Iterator[ollama._types.ChatResponse]] method of ollama._client.Client instance\n",
      "    Create a chat response using the requested model.\n",
      "    \n",
      "    Args:\n",
      "      tools:\n",
      "        A JSON schema as a dict, an Ollama Tool or a Python Function.\n",
      "        Python functions need to follow Google style docstrings to be converted to an Ollama Tool.\n",
      "        For more information, see: https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings\n",
      "      stream: Whether to stream the response.\n",
      "      format: The format of the response.\n",
      "    \n",
      "    Example:\n",
      "      def add_two_numbers(a: int, b: int) -> int:\n",
      "        '''\n",
      "        Add two numbers together.\n",
      "    \n",
      "        Args:\n",
      "          a: First number to add\n",
      "          b: Second number to add\n",
      "    \n",
      "        Returns:\n",
      "          int: The sum of a and b\n",
      "        '''\n",
      "        return a + b\n",
      "    \n",
      "      client.chat(model='llama3.2', tools=[add_two_numbers], messages=[...])\n",
      "    \n",
      "    Raises `RequestError` if a model is not provided.\n",
      "    \n",
      "    Raises `ResponseError` if the request could not be fulfilled.\n",
      "    \n",
      "    Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ztm_ai_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
